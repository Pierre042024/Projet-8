{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61628f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy\n",
      "  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\puech\\anaconda3\\lib\\site-packages (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\puech\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy)\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\puech\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
      "   ---------------------------------------- 0.0/54.4 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 30.7/54.4 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 54.4/54.4 kB 940.3 kB/s eta 0:00:00\n",
      "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Installing collected packages: wcwidth, ftfy\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.5\n",
      "    Uninstalling wcwidth-0.2.5:\n",
      "      Successfully uninstalled wcwidth-0.2.5\n",
      "Successfully installed ftfy-6.2.0 wcwidth-0.2.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\puech\\appdata\\local\\temp\\pip-req-build-1kcwf5hi\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in c:\\users\\puech\\anaconda3\\lib\\site-packages (from clip==1.0) (6.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\puech\\anaconda3\\lib\\site-packages (from clip==1.0) (23.1)\n",
      "Requirement already satisfied: regex in c:\\users\\puech\\anaconda3\\lib\\site-packages (from clip==1.0) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\puech\\anaconda3\\lib\\site-packages (from clip==1.0) (4.65.0)\n",
      "Requirement already satisfied: torch in c:\\users\\puech\\anaconda3\\lib\\site-packages (from clip==1.0) (2.2.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\puech\\anaconda3\\lib\\site-packages (from clip==1.0) (0.17.1)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in c:\\users\\puech\\anaconda3\\lib\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in c:\\users\\puech\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\puech\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\puech\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\puech\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\puech\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\puech\\anaconda3\\lib\\site-packages (from torch->clip==1.0) (2024.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\puech\\anaconda3\\lib\\site-packages (from torchvision->clip==1.0) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\puech\\anaconda3\\lib\\site-packages (from torchvision->clip==1.0) (9.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\puech\\anaconda3\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\puech\\anaconda3\\lib\\site-packages (from jinja2->torch->clip==1.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\puech\\anaconda3\\lib\\site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py): started\n",
      "  Building wheel for clip (setup.py): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369572 sha256=e11f7f62d952b8681de77872b6c7ac8144b3063cf75ef8db27a2f80255005631\n",
      "  Stored in directory: C:\\Users\\puech\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-l87xjk6y\\wheels\\3f\\7c\\a4\\9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\puech\\AppData\\Local\\Temp\\pip-req-build-1kcwf5hi'\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "789e338c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69bad5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9fd90c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 338M/338M [07:07<00:00, 829kiB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Charger le modèle CLIP et les transformations d'images\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Chemin vers les images et fichier CSV\n",
    "image_path = r\"C:\\Users\\puech\\Documents\\Flipkart\\Images\"\n",
    "data = pd.read_csv('flipkart_com-ecommerce_sample_1050.csv')\n",
    "\n",
    "# Prétraiter les catégories de produits\n",
    "data['label_name'] = data['product_category_tree'].apply(lambda x: x.split('>>')[0].strip())\n",
    "data['label_name'] = data['label_name'].str.replace(\"[\", \"\").str.strip()\n",
    "data = data[['image', 'label_name']]\n",
    "\n",
    "# Créer une liste des chemins d'images et des étiquettes correspondantes\n",
    "image_files = [os.path.join(image_path, img) for img in data['image']]\n",
    "labels = data['label_name'].tolist()\n",
    "\n",
    "# Encodage des étiquettes en texte\n",
    "text_labels = clip.tokenize(labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d5c220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\puech\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Créer les embeddings pour les étiquettes textuelles\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text_labels)\n",
    "\n",
    "# Créer les embeddings pour les images\n",
    "image_features = []\n",
    "for image_file in image_files:\n",
    "    image = preprocess(Image.open(image_file)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features.append(model.encode_image(image))\n",
    "\n",
    "image_features = torch.stack(image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f15bf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions des embeddings d'images : torch.Size([1050, 1, 512])\n",
      "Dimensions des embeddings de textes : torch.Size([1050, 512])\n",
      "Dimensions des similarités : torch.Size([1050, 1, 1050])\n",
      "Dimensions des indices : torch.Size([1050, 1050])\n",
      "Nombre d'indices : 1102500\n",
      "Nombre de lignes dans le DataFrame : 1050\n"
     ]
    }
   ],
   "source": [
    "# Vérifier les dimensions des embeddings d'images et de textes\n",
    "print(f\"Dimensions des embeddings d'images : {image_features.shape}\")\n",
    "print(f\"Dimensions des embeddings de textes : {text_features.shape}\")\n",
    "\n",
    "# Calculer les similarités cosinus entre les embeddings d'images et de textes\n",
    "similarities = image_features @ text_features.T\n",
    "print(f\"Dimensions des similarités : {similarities.shape}\")\n",
    "\n",
    "# Trouver les indices des étiquettes les plus similaires\n",
    "_, indices = similarities.max(dim=1)\n",
    "print(f\"Dimensions des indices : {indices.shape}\")\n",
    "\n",
    "# Convertir les indices en une liste d'entiers\n",
    "indices = indices.cpu().numpy().flatten().tolist()\n",
    "\n",
    "# Vérifier la longueur des indices et du DataFrame\n",
    "print(f\"Nombre d'indices : {len(indices)}\")\n",
    "print(f\"Nombre de lignes dans le DataFrame : {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4e60dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions corrigées des embeddings d'images : torch.Size([1050, 512])\n",
      "Dimensions des similarités : torch.Size([1050, 1050])\n",
      "Dimensions des indices : torch.Size([1050])\n",
      "Nombre d'indices : 1050\n",
      "Nombre de lignes dans le DataFrame : 1050\n",
      "                                  image        label_name  \\\n",
      "0  55b85ea15a1536d46b7190ad6fff8ce7.jpg  \"Home Furnishing   \n",
      "1  7b72c92c2f6c40268628ec5f14c6d590.jpg        \"Baby Care   \n",
      "2  64d5d4a258243731dc7bbb1eef49ad74.jpg        \"Baby Care   \n",
      "3  d4684dcdc759dd9cdf41504698d737d8.jpg  \"Home Furnishing   \n",
      "4  6325b6870c54cd47be6ebfbffa620ec7.jpg  \"Home Furnishing   \n",
      "\n",
      "               predicted_label  \n",
      "0             \"Home Furnishing  \n",
      "1  \"Home Decor & Festive Needs  \n",
      "2             \"Home Furnishing  \n",
      "3             \"Home Furnishing  \n",
      "4             \"Home Furnishing  \n"
     ]
    }
   ],
   "source": [
    "# S'assurer que la dimension des embeddings d'images est correcte\n",
    "image_features = image_features.squeeze()\n",
    "\n",
    "# Vérifier les dimensions des embeddings après correction\n",
    "print(f\"Dimensions corrigées des embeddings d'images : {image_features.shape}\")\n",
    "\n",
    "# Calculer les similarités cosinus entre les embeddings d'images et de textes\n",
    "similarities = image_features @ text_features.T\n",
    "print(f\"Dimensions des similarités : {similarities.shape}\")\n",
    "\n",
    "# Trouver les indices des étiquettes les plus similaires\n",
    "_, indices = similarities.max(dim=1)\n",
    "print(f\"Dimensions des indices : {indices.shape}\")\n",
    "\n",
    "# Convertir les indices en une liste d'entiers\n",
    "indices = indices.cpu().numpy().flatten().tolist()\n",
    "\n",
    "# Vérifier la longueur des indices et du DataFrame\n",
    "print(f\"Nombre d'indices : {len(indices)}\")\n",
    "print(f\"Nombre de lignes dans le DataFrame : {len(data)}\")\n",
    "\n",
    "# Assurer que chaque élément de indices est un entier\n",
    "indices = [int(i) for i in indices]\n",
    "\n",
    "# Mapper les indices aux étiquettes\n",
    "predicted_labels = [labels[i] for i in indices]\n",
    "\n",
    "# Assurer que la longueur des étiquettes prédites correspond au DataFrame\n",
    "assert len(predicted_labels) == len(data), \"La longueur des étiquettes prédites ne correspond pas à celle du DataFrame\"\n",
    "\n",
    "# Ajouter les étiquettes prédites au DataFrame\n",
    "data['predicted_label'] = predicted_labels\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c75ac2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.05%\n"
     ]
    }
   ],
   "source": [
    "# Évaluation de la précision\n",
    "accuracy = (data['label_name'] == data['predicted_label']).mean()\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6056238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement (80%) et de test (20%)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, stratify=data['label_name'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a533d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\puech\\Documents\\Flipkart\\Images\"\n",
    "\n",
    "# Embeddings pour l'ensemble d'entraînement\n",
    "train_image_features = []\n",
    "for image_path in train_data['image']:\n",
    "    image = preprocess(Image.open(os.path.join(path, image_path))).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "    train_image_features.append(image_features.cpu().numpy())\n",
    "train_image_features = np.vstack(train_image_features)\n",
    "\n",
    "train_texts = [f\"Cette image appartient à la catégorie {label}\" for label in train_data['label_name']]\n",
    "with torch.no_grad():\n",
    "    train_text_features = model.encode_text(clip.tokenize(train_texts).to(device)).detach().cpu().numpy()\n",
    "\n",
    "# Embeddings pour l'ensemble de test\n",
    "test_image_features = []\n",
    "for image_path in test_data['image']:\n",
    "    image = preprocess(Image.open(os.path.join(path, image_path))).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "    test_image_features.append(image_features.cpu().numpy())\n",
    "test_image_features = np.vstack(test_image_features)\n",
    "\n",
    "test_texts = [f\"Cette image appartient à la catégorie {label}\" for label in test_data['label_name']]\n",
    "with torch.no_grad():\n",
    "    test_text_features = model.encode_text(clip.tokenize(test_texts).to(device)).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76c50997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.38%\n"
     ]
    }
   ],
   "source": [
    "# Calculer les similarités cosinus entre les embeddings des images de test et des textes d'entraînement\n",
    "similarities = test_image_features @ train_text_features.T\n",
    "\n",
    "# Trouver les indices des étiquettes les plus similaires pour chaque image de test\n",
    "indices = similarities.argmax(axis=1)\n",
    "\n",
    "# Mapper les indices aux étiquettes d'entraînement\n",
    "predicted_labels = [train_data.iloc[i]['label_name'] for i in indices]\n",
    "\n",
    "# Ajouter les étiquettes prédites au DataFrame de test\n",
    "test_data['predicted_label'] = predicted_labels\n",
    "\n",
    "# Évaluation de la précision\n",
    "accuracy = (test_data['label_name'] == test_data['predicted_label']).mean()\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f0a5863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\puech\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3167: DecompressionBombWarning: Image size (93680328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 64.17%\n",
      "Test Accuracy: 62.38%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement (80%) et de test (20%)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, stratify=data['label_name'], random_state=42)\n",
    "\n",
    "# Embeddings pour l'ensemble d'entraînement\n",
    "train_image_features = []\n",
    "for image_path in train_data['image']:\n",
    "    image = preprocess(Image.open(os.path.join(path, image_path))).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "    train_image_features.append(image_features.cpu().numpy())\n",
    "train_image_features = np.vstack(train_image_features)\n",
    "\n",
    "train_texts = [f\"Cette image appartient à la catégorie {label}\" for label in train_data['label_name']]\n",
    "with torch.no_grad():\n",
    "    train_text_features = model.encode_text(clip.tokenize(train_texts).to(device)).detach().cpu().numpy()\n",
    "\n",
    "# Embeddings pour l'ensemble de test\n",
    "test_image_features = []\n",
    "for image_path in test_data['image']:\n",
    "    image = preprocess(Image.open(os.path.join(path, image_path))).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "    test_image_features.append(image_features.cpu().numpy())\n",
    "test_image_features = np.vstack(test_image_features)\n",
    "\n",
    "test_texts = [f\"Cette image appartient à la catégorie {label}\" for label in test_data['label_name']]\n",
    "with torch.no_grad():\n",
    "    test_text_features = model.encode_text(clip.tokenize(test_texts).to(device)).detach().cpu().numpy()\n",
    "\n",
    "# Calculer les similarités cosinus entre les embeddings des images d'entraînement et des textes d'entraînement\n",
    "train_similarities = train_image_features @ train_text_features.T\n",
    "train_indices = train_similarities.argmax(axis=1)\n",
    "train_predicted_labels = [train_data.iloc[i]['label_name'] for i in train_indices]\n",
    "train_data['predicted_label'] = train_predicted_labels\n",
    "\n",
    "# Évaluation de la précision pour les données d'entraînement\n",
    "train_accuracy = (train_data['label_name'] == train_data['predicted_label']).mean()\n",
    "print(f\"Train Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Calculer les similarités cosinus entre les embeddings des images de test et des textes d'entraînement\n",
    "test_similarities = test_image_features @ train_text_features.T\n",
    "test_indices = test_similarities.argmax(axis=1)\n",
    "test_predicted_labels = [train_data.iloc[i]['label_name'] for i in test_indices]\n",
    "test_data['predicted_label'] = test_predicted_labels\n",
    "\n",
    "# Évaluation de la précision pour les données de test\n",
    "test_accuracy = (test_data['label_name'] == test_data['predicted_label']).mean()\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6493dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
